{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iXyvHO25nxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe8ea450-f44a-48ee-d3ac-ac5189637c9e"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9RwQF0p3VHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8aadc8ed-efe8-434d-a2df-629c6fef0827"
      },
      "source": [
        "!unzip MNIST_data.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  MNIST_data.zip\n",
            "   creating: MNIST_data/\n",
            "  inflating: __MACOSX/._MNIST_data   \n",
            "  inflating: MNIST_data/t10k-images-idx3-ubyte.gz  \n",
            "  inflating: __MACOSX/MNIST_data/._t10k-images-idx3-ubyte.gz  \n",
            "  inflating: MNIST_data/train-images-idx3-ubyte.gz  \n",
            "  inflating: __MACOSX/MNIST_data/._train-images-idx3-ubyte.gz  \n",
            "  inflating: MNIST_data/train-labels-idx1-ubyte.gz  \n",
            "  inflating: __MACOSX/MNIST_data/._train-labels-idx1-ubyte.gz  \n",
            "  inflating: MNIST_data/t10k-labels-idx1-ubyte.gz  \n",
            "  inflating: __MACOSX/MNIST_data/._t10k-labels-idx1-ubyte.gz  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2WObQLb519Y",
        "colab_type": "text"
      },
      "source": [
        "#Dense Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PO6qHM7d8S",
        "colab_type": "text"
      },
      "source": [
        "**Problem 1**. Consider the following neural network:\n",
        "    $$h_1 =W_1 X+b_1$$\n",
        "    $$a_1 =sigmoid(h_1)$$\n",
        "    $$h_2 =W_2 a_1+b_2$$\n",
        "    $$a_2 =tanh(h_2)$$\n",
        "    $$o=W_3a_2+b_3$$\n",
        "    $$p=softmax(o)$$\n",
        "    $$softmax(o_i)=\\frac{e^{o_i}}{\\sum_{j=0}^{K}e^{o_j}} \\qquad i=0,1,...,K$$\n",
        "where, $$h_n$$ denote the hidden layers, $a_n$ denotes the activation layers, $W_n$ are the weights, X being the input to the neural network, o denotes the output layer and p denotes the predicted probabilities. The cross-entropy loss is used as the loss function and is given by:  \n",
        "$$L=- \\sum_{\\text{for all class c}}y_c log(p_c)$$ \n",
        "\n",
        "where y is the target labels (one-hot vector, i.e., the $y_c$ = 1 if the label of the instance is c).\n",
        "Compute the derivative of the cross-entropy loss L w.r.t o, i.e compute $\\frac{\\partial L }{\\partial o}$. (25 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82tHRYfk9V0_",
        "colab_type": "text"
      },
      "source": [
        "**Problem 2**. Draw the computational graph of the network described in Problem 1. Using\n",
        "the derivative calculated in the previous question, perform Backpropagation to compute the\n",
        "gradients of loss L w.r.t all the weights and biases, i.e compute $\\frac{\\partial L }{\\partial W_1},\\frac{\\partial L }{\\partial W_2},\\frac{\\partial L }{\\partial W_3},\\frac{\\partial L }{\\partial b_1},\\frac{\\partial L }{\\partial b_2},\\frac{\\partial L }{\\partial b_3}$\n",
        ". (25 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM69jiFh5yz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n",
        "\n",
        "def fully_connected(prev_layer, num_units, batch_norm, is_training=False):\n",
        "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
        "    if batch_norm:\n",
        "    \tlayer = tf.layers.batch_normalization(layer, training=is_training)\n",
        "    layer = tf.nn.relu(layer)\n",
        "    return layer\n",
        "\n",
        "num_batches = 3000\n",
        "batch_size = 128\n",
        "learning_rate = 0.002\n",
        "batch_norm = True\n",
        "layer_num = 5\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "labels = tf.placeholder(tf.float32, [None, 10])\n",
        "is_training = tf.placeholder(tf.bool)\n",
        "\n",
        "layer = inputs\n",
        "orig_shape = layer.get_shape().as_list()\n",
        "layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
        "for layer_i in range(1, layer_num + 1):\n",
        "    layer = fully_connected(layer, 2**(9-layer_i), batch_norm, is_training)\n",
        "\n",
        "logits = tf.layers.dense(layer, 10)\n",
        "    \n",
        "model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "tf.summary.scalar('fc_loss',model_loss)\n",
        "\n",
        "if batch_norm:  \n",
        "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "\t    #train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
        "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
        "        #train_opt = tf.train.RMSPropOptimizer(learning_rate).minimize(model_loss)\n",
        "else:\n",
        "\t#train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
        "    #train_opt = tf.train.RMSPropOptimize(learning_rate).minimize(model_loss)\n",
        "\ttrain_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "with tf.Session() as sess:\n",
        "    merged = tf.summary.merge_all()\n",
        "    if batch_norm: \n",
        "        logdir = \"mnist/fc/SGD_batchnorm\"\n",
        "    else:\n",
        "        logdir = \"mnist/fc/SGD_no_batchnorm\"\n",
        "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for batch_i in range(num_batches):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        _,summary = sess.run([train_opt,merged], {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
        "        \n",
        "        writer.add_summary(summary, batch_i)\n",
        "\n",
        "        if batch_i % 500 == 0:\n",
        "            loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
        "                labels: mnist.validation.labels,\n",
        "                is_training: False})\n",
        "            print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
        "        elif batch_i % 100 == 0:\n",
        "            loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
        "            print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
        "\n",
        "            # At the end, score the final accuracy for both the validation and test sets\n",
        "    acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
        "        labels: mnist.validation.labels,is_training: False})\n",
        "    print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
        "    acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
        "        labels: mnist.test.labels,is_training: False})\n",
        "    print('Final test accuracy: {:>3.5f}'.format(acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDnDzbWT9vIb",
        "colab_type": "text"
      },
      "source": [
        "**Problem 3**. Batch Normalization is a technique that forces the input to any layer to be\n",
        "zero mean and unit standard deviation. Below is the algorithm from the paper (Refer: https://arxiv.org/pdf/1502.03167.pdf), Using the algorithm, draw the computational graph of the batchnorm layer. Consider a function $F(y_i)$ and compute the derivatives of\n",
        "$F(y_i)$ w.r.t x, γ and β, i.e compute\n",
        "$\\frac{\\partial F }{\\partial \\gamma},\\frac{\\partial F }{\\partial \\beta},\\frac{\\partial F }{\\partial x}$,Assume  $\\frac{\\partial F }{\\partial y_i}$\n",
        "is given. (25 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBUm_zrR-V6L",
        "colab_type": "text"
      },
      "source": [
        "**Problem 4**. Consider the MNIST dataset. It consists of 10 class labels (0-9) and has 60,000\n",
        "training images and 10,000 test images.\n",
        "1. Construct a model using fully connected layers (at least 3 layers or more!) and ReLu layers to solve this classification problem using Tensorflow. Report the accuracy obtained on the test set. Plot a graph demonstrating how the loss function decreases\n",
        "over the number of iterations. (5 marks)\n",
        "\n",
        "2. Add batch normalization layers in the model. Report the accuracy obtained and plot\n",
        "a graph showing how loss decreases. Elaborate briefly on how and why batch normalization helped. (5 marks)\n",
        "\n",
        "3. For the same dataset, train a Convolutional Neural Network (with and without batchnorm). Try experimenting with different architectures (different optimizers, number of\n",
        "convolutional layers, etc) and report the accuracy that you obtained with both using\n",
        "and without using batchnorm. Plot the loss vs iterations graph and explain why and\n",
        "how batch normalization helped. (15 marks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ULJrgL5A1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "c66d3202-1d4d-4de2-de90-7ffd9538d81f"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)\n",
        "\n",
        "def fully_connected(prev_layer, num_units, batch_norm, is_training=False):\n",
        "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
        "    if batch_norm:\n",
        "        layer = tf.layers.batch_normalization(layer, training=is_training)\n",
        "    layer = tf.nn.relu(layer)\n",
        "    return layer\n",
        "\n",
        "def conv_layer(prev_layer, layer_depth, batch_norm, is_training=False):\n",
        "\tif layer_depth % 3 == 0:\n",
        "\t    strides = 2\n",
        "\telse:\n",
        "\t\tstrides = 1\n",
        "\tconv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', use_bias=False, activation=None)\n",
        "\tif batch_norm:\n",
        "\t\tconv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
        "\tconv_layer = tf.nn.relu(conv_layer)\n",
        "\treturn conv_layer\n",
        "\n",
        "\n",
        "num_batches = 3000\n",
        "batch_size = 128\n",
        "learning_rate = 0.002\n",
        "layer_num = 5\n",
        "batch_norm = True\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "labels = tf.placeholder(tf.float32, [None, 10])\n",
        "is_training = tf.placeholder(tf.bool)\n",
        "\n",
        "layer = inputs\n",
        "for layer_i in range(1, 1+layer_num):\n",
        "    layer = conv_layer(layer, layer_i, batch_norm, is_training)\n",
        "\n",
        "orig_shape = layer.get_shape().as_list()\n",
        "\n",
        "layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
        "layer = fully_connected(layer, 100, batch_norm, is_training)\n",
        "\n",
        "logits = tf.layers.dense(layer, 10)\n",
        "model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "tf.summary.scalar('conv_loss',model_loss)\n",
        "\n",
        "if batch_norm:  \n",
        "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "        #train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
        "\t\t#train_opt = tf.train.RMSPropOptimize(learning_rate).minimize(model_loss)\n",
        "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
        "else:\n",
        "    train_opt = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
        "\t#train_opt = tf.train.RMSPropOptimize(learning_rate).minimize(model_loss)\n",
        "\t#train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "\n",
        "with tf.Session() as sess:\n",
        "\tmerged = tf.summary.merge_all()\n",
        "\tif batch_norm: \n",
        "\t\tlogdir = \"mnist/conv/SGD_batchnorm\"\n",
        "\telse:\n",
        "\t\tlogdir = \"mnist/conv/SGD_no_batchnorm\"\n",
        "\twriter = tf.summary.FileWriter(logdir, sess.graph)\n",
        "\n",
        "\tsess.run(tf.global_variables_initializer())\n",
        "\tfor batch_i in range(num_batches):\n",
        "\t\tbatch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "\t\t_,summary = sess.run([train_opt,merged], {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
        "\t\t\n",
        "\t\twriter.add_summary(summary, batch_i)\n",
        "\n",
        "\t\tif batch_i % 500 == 0:\n",
        "\t\t\tloss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images, labels: mnist.validation.labels, is_training: False})\n",
        "\t\t\tprint('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
        "\t\telif batch_i % 100 == 0:\n",
        "\t\t\tloss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
        "\t\t\tprint('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
        "\n",
        "\tacc = sess.run(accuracy, {inputs: mnist.validation.images, labels: mnist.validation.labels,is_training: False})\n",
        "\tprint('Final validation accuracy: {:>3.5f}'.format(acc))\n",
        "\tacc = sess.run(accuracy, {inputs: mnist.test.images, labels: mnist.test.labels,is_training: False})\n",
        "\tprint('Final test accuracy: {:>3.5f}'.format(acc))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Batch:  0: Validation loss: 0.69016, Validation accuracy: 0.31800\n",
            "Batch: 100: Training loss: 0.39585, Training accuracy: 0.09375\n",
            "Batch: 200: Training loss: 0.27462, Training accuracy: 0.40625\n",
            "Batch: 300: Training loss: 0.13369, Training accuracy: 0.78125\n",
            "Batch: 400: Training loss: 0.03592, Training accuracy: 0.96094\n",
            "Batch: 500: Validation loss: 0.01450, Validation accuracy: 0.98200\n",
            "Batch: 600: Training loss: 0.00820, Training accuracy: 0.99219\n",
            "Batch: 700: Training loss: 0.00824, Training accuracy: 0.98438\n",
            "Batch: 800: Training loss: 0.00861, Training accuracy: 0.99219\n",
            "Batch: 900: Training loss: 0.00919, Training accuracy: 0.98438\n",
            "Batch: 1000: Validation loss: 0.00868, Validation accuracy: 0.98780\n",
            "Batch: 1100: Training loss: 0.00185, Training accuracy: 1.00000\n",
            "Batch: 1200: Training loss: 0.01392, Training accuracy: 0.96875\n",
            "Batch: 1300: Training loss: 0.00216, Training accuracy: 1.00000\n",
            "Batch: 1400: Training loss: 0.00177, Training accuracy: 1.00000\n",
            "Batch: 1500: Validation loss: 0.00657, Validation accuracy: 0.99060\n",
            "Batch: 1600: Training loss: 0.00196, Training accuracy: 1.00000\n",
            "Batch: 1700: Training loss: 0.00358, Training accuracy: 1.00000\n",
            "Batch: 1800: Training loss: 0.00157, Training accuracy: 1.00000\n",
            "Batch: 1900: Training loss: 0.00151, Training accuracy: 1.00000\n",
            "Batch: 2000: Validation loss: 0.00934, Validation accuracy: 0.98620\n",
            "Batch: 2100: Training loss: 0.00072, Training accuracy: 1.00000\n",
            "Batch: 2200: Training loss: 0.00201, Training accuracy: 1.00000\n",
            "Batch: 2300: Training loss: 0.00348, Training accuracy: 0.99219\n",
            "Batch: 2400: Training loss: 0.00194, Training accuracy: 1.00000\n",
            "Batch: 2500: Validation loss: 0.00660, Validation accuracy: 0.99040\n",
            "Batch: 2600: Training loss: 0.00115, Training accuracy: 1.00000\n",
            "Batch: 2700: Training loss: 0.00071, Training accuracy: 1.00000\n",
            "Batch: 2800: Training loss: 0.00121, Training accuracy: 1.00000\n",
            "Batch: 2900: Training loss: 0.00121, Training accuracy: 1.00000\n",
            "Final validation accuracy: 0.98940\n",
            "Final test accuracy: 0.98910\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}